{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/erland.fuadi/Python Project/supertrainer\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-18 23:09:53.834\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msupertrainer.utils.helpers\u001b[0m:\u001b[36mlogin_hf\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mUse token from environment variable HUGGINGFACE_API_KEY\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/erland.fuadi/.cache/huggingface/token\n",
      "Login successful\n",
      "{'trainer': {'testing': False, 'model_kwargs': {'attn_implementation': 'flash_attention_2', 'torch_dtype': 'auto', 'use_cache': False, 'device_map': 'auto'}, 'bitsandbytes_kwargs': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': None}, 'training_kwargs': {'gradient_accumulation_steps': 64, 'warmup_steps': 5, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'learning_rate': 2e-05, 'fp16': False, 'bf16': True, 'logging_steps': 20, 'logging_strategy': 'steps', 'optim': 'adamw_8bit', 'weight_decay': 0.01, 'lr_scheduler_type': 'linear', 'seed': 3407, 'report_to': 'wandb', 'output_dir': './assets_local/smollm/outputs', 'load_best_model_at_end': 1, 'metric_for_best_model': 'eval_loss', 'save_total_limit': 2, 'eval_strategy': 'steps', 'eval_steps': 200, 'save_steps': 400, 'eval_on_start': True, 'hub_model_id': 'Erland/llm-preskripsi', 'hub_private_repo': True, 'hub_strategy': 'every_save', 'push_to_hub': True, 'auto_find_batch_size': True, 'include_tokens_per_second': True, 'include_num_input_tokens_seen': True, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': {'use_reentrant': False}}, 'peft_kwargs': {'r': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], 'lora_alpha': 16, 'lora_dropout': 0, 'bias': 'none', 'use_gradient_checkpointing': 'unsloth', 'random_state': 3407, 'use_rslora': False, 'loftq_config': None}, 'class_name': 'supertrainer.trainers.llm_trainer.LLMTrainer', 'load_config': {'max_seq_length': 4096, 'dtype': None, 'load_in_4bit': True, 'model_name': 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit'}, 'model_name': 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit'}, 'dataset': {'testing': False, 'class_name': 'supertrainer.data.llm.ConversationLLMDataset', 'dataset_kwargs': {'path': 'Erland/llm_sample_dataset', 'tokenizer_name_or_path': 'unsloth/Meta-Llama-3.1-8B-Instruct'}}}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from supertrainer import StrictDict\n",
    "from supertrainer.utils.helpers import login_hf\n",
    "\n",
    "load_dotenv()\n",
    "login_hf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'class_name': 'supertrainer.data.supertrainer.SupertrainerBERTDataset',\n",
      "             'dataset_kwargs': {'path': 'masa-research/manual_absa_annotation__dgv__20240903_103',\n",
      "                                'tokenizer_name_or_path': 'markussagen/xlm-roberta-longformer-base-4096'},\n",
      "             'testing': False},\n",
      " 'trainer': {'bitsandbytes_kwargs': {'bnb_4bit_compute_dtype': None,\n",
      "                                     'bnb_4bit_quant_type': 'nf4',\n",
      "                                     'load_in_4bit': True},\n",
      "             'class_name': 'supertrainer.trainers.hf_trainer.HuggingFaceBERTTrainer',\n",
      "             'classes': ['positif', 'negatif', 'netral'],\n",
      "             'model_kwargs': {'attn_implementation': 'flash_attention_2',\n",
      "                              'device_map': 'auto',\n",
      "                              'torch_dtype': 'auto',\n",
      "                              'use_cache': False},\n",
      "             'model_name': 'markussagen/xlm-roberta-longformer-base-4096',\n",
      "             'peft_kwargs': {'bias': 'none',\n",
      "                             'loftq_config': None,\n",
      "                             'lora_alpha': 16,\n",
      "                             'lora_dropout': 0,\n",
      "                             'r': 64,\n",
      "                             'target_modules': ['query',\n",
      "                                                'key',\n",
      "                                                'value',\n",
      "                                                'dense'],\n",
      "                             'task_type': 'SEQ_CLS',\n",
      "                             'use_rslora': True},\n",
      "             'testing': False,\n",
      "             'training_kwargs': {'auto_find_batch_size': True,\n",
      "                                 'bf16': True,\n",
      "                                 'eval_on_start': True,\n",
      "                                 'eval_steps': 200,\n",
      "                                 'eval_strategy': 'steps',\n",
      "                                 'fp16': False,\n",
      "                                 'gradient_accumulation_steps': 64,\n",
      "                                 'gradient_checkpointing': True,\n",
      "                                 'gradient_checkpointing_kwargs': {'use_reentrant': False},\n",
      "                                 'hub_model_id': 'masa-research/bert-preskripsi',\n",
      "                                 'hub_private_repo': True,\n",
      "                                 'hub_strategy': 'every_save',\n",
      "                                 'include_num_input_tokens_seen': True,\n",
      "                                 'include_tokens_per_second': True,\n",
      "                                 'learning_rate': 2e-05,\n",
      "                                 'load_best_model_at_end': 1,\n",
      "                                 'logging_steps': 20,\n",
      "                                 'logging_strategy': 'steps',\n",
      "                                 'lr_scheduler_type': 'linear',\n",
      "                                 'metric_for_best_model': 'eval_loss',\n",
      "                                 'num_train_epochs': 3,\n",
      "                                 'optim': 'adamw_8bit',\n",
      "                                 'output_dir': './assets_local/smollm/outputs',\n",
      "                                 'per_device_eval_batch_size': 1,\n",
      "                                 'per_device_train_batch_size': 4,\n",
      "                                 'push_to_hub': True,\n",
      "                                 'report_to': 'wandb',\n",
      "                                 'save_steps': 400,\n",
      "                                 'save_total_limit': 2,\n",
      "                                 'seed': 3407,\n",
      "                                 'warmup_steps': 5,\n",
      "                                 'weight_decay': 0.01}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "with initialize(config_path=\"../configs\", version_base=None):\n",
    "    cfg = compose(config_name=\"train\", overrides=[\"+experiment=train_longformer\"])\n",
    "    cfg = OmegaConf.to_container(cfg, resolve=True)\n",
    "    cfg = StrictDict(cfg)\n",
    "    pprint(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 4096\n",
    "MODEL_NAME_OR_PATH = \"markussagen/xlm-roberta-longformer-base-4096\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_OR_PATH, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Article Reference', 'Article ID', 'Title', 'Content', 'Date', 'Article Media', 'Article Submedia', 'Journalist', 'Entity', 'Entity Sentiment', 'Difficulty', 'General Sentiment', 'Notes', 'Label'],\n",
       "        num_rows: 1076\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Article Reference', 'Article ID', 'Title', 'Content', 'Date', 'Article Media', 'Article Submedia', 'Journalist', 'Entity', 'Entity Sentiment', 'Difficulty', 'General Sentiment', 'Notes', 'Label'],\n",
       "        num_rows: 62\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Article Reference', 'Article ID', 'Title', 'Content', 'Date', 'Article Media', 'Article Submedia', 'Journalist', 'Entity', 'Entity Sentiment', 'Difficulty', 'General Sentiment', 'Notes', 'Label'],\n",
       "        num_rows: 216\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"masa-research/absa_news__1.5k__20240923_154258\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
