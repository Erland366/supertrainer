{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/erland.fuadi/Python Project/supertrainer\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-17 18:48:42.394\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msupertrainer.utils.helpers\u001b[0m:\u001b[36mlogin_hf\u001b[0m:\u001b[36m18\u001b[0m - \u001b[34m\u001b[1mUse token from environment variable HUGGINGFACE_API_KEY\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/erland.fuadi/.cache/huggingface/token\n",
      "Login successful\n",
      "{'trainer': {'model_kwargs': {'attn_implementation': 'flash_attention_2', 'torch_dtype': 'auto', 'use_cache': False, 'device_map': None}, 'bitsandbytes_kwargs': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': None}, 'training_kwargs': {'gradient_accumulation_steps': 64, 'warmup_steps': 5, 'num_train_epochs': 3, 'learning_rate': 2e-05, 'fp16': False, 'bf16': True, 'logging_steps': 20, 'logging_strategy': 'steps', 'optim': 'adamw_8bit', 'weight_decay': 0.01, 'lr_scheduler_type': 'linear', 'seed': 3407, 'report_to': 'wandb', 'output_dir': './assets_local/smollm/outputs', 'load_best_model_at_end': 1, 'metric_for_best_model': 'eval_loss', 'save_total_limit': 2, 'eval_strategy': 'steps', 'eval_steps': 200, 'save_steps': 400, 'eval_on_start': True, 'hub_model_id': 'masa-research/llm-preskripsi', 'hub_private_repo': True, 'hub_strategy': 'every_save', 'push_to_hub': True, 'auto_find_batch_size': True, 'include_tokens_per_second': True, 'include_num_input_tokens_seen': True, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': {'use_reentrant': False}}, 'peft_kwargs': {}, 'class_name': 'preskripsi_training.trainers.hf_trainer.HuggingFaceBERTTrainer', 'model_name': 'bert-base-uncased'}, 'dataset': {'class_name': 'supertrainer.data.llm.ConversationLLMDataset', 'dataset_kwargs': {'path': 'Erland/llm_sample_dataset', 'tokenizer_name_or_path': 'unsloth/Meta-Llama-3.1-8B-Instruct'}}}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from supertrainer import StrictDict\n",
    "from supertrainer.utils.helpers import login_hf\n",
    "\n",
    "load_dotenv()\n",
    "login_hf()\n",
    "\n",
    "with initialize(config_path=\"../configs\", version_base=None):\n",
    "    cfg = compose(config_name=\"train\", overrides=[\"+experiment=train_llm\"])\n",
    "    cfg = OmegaConf.to_container(cfg, resolve=True)\n",
    "    cfg = StrictDict(cfg)\n",
    "    print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-17 17:19:08.517\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msupertrainer.utils.helpers\u001b[0m:\u001b[36msplit_train_test_validation\u001b[0m:\u001b[36m199\u001b[0m - \u001b[33m\u001b[1mDataset is a DatasetDict. We will you're going to use the 'train' dataset.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from supertrainer.utils.helpers import split_train_test_validation\n",
    "\n",
    "split_dataset = split_train_test_validation(dataset, select_subset=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c672183f67ee44aeb2ca0262b9fa2bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a6fd4f2b42419996588f30714f988d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5794b8547804618b3ca8837a404dafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0bcd505291416483ec4eea9507f46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f1d007b055439db32deb062d9bcea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b67f09cfc754b67b48a2aa797c8fc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Erland/llm_sample_dataset/commit/bc90b7fd6aa755a61f6d403533e86f91417e5b81', commit_message='Upload dataset', commit_description='', oid='bc90b7fd6aa755a61f6d403533e86f91417e5b81', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "split_dataset.push_to_hub(\"Erland/llm_sample_dataset\", private=True, token=os.getenv(\"HUGGINGFACE_API_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<supertrainer.data.llm.ConversationLLMDataset at 0x14a9b2e56a10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from supertrainer.data.llm import ConversationLLMDataset\n",
    "\n",
    "dataset = ConversationLLMDataset(config=cfg[\"dataset\"], is_testing=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-17 18:49:14.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msupertrainer.data.llm\u001b[0m:\u001b[36mprepare_dataset\u001b[0m:\u001b[36m178\u001b[0m - \u001b[34m\u001b[1mPreparing dataset\u001b[0m\n",
      "\u001b[32m2024-09-17 18:49:14.244\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msupertrainer.data.llm\u001b[0m:\u001b[36mprepare_dataset\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1mApplying formatting prompts\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConversationLLMDataset' object has no attribute 'format_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m formatted_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m formatted_dataset\n",
      "File \u001b[0;32m~/Python Project/supertrainer/src/supertrainer/data/llm.py:188\u001b[0m, in \u001b[0;36mConversationLLMDataset.prepare_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying formatting prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 188\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_dataset\u001b[49m(dataset)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_tokenization(dataset)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Print some examples from the dataset to inspect the tokenizer's output\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConversationLLMDataset' object has no attribute 'format_dataset'"
     ]
    }
   ],
   "source": [
    "formatted_dataset = dataset.prepare_dataset()\n",
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 199\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 63\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
