{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Metrics DataFrame:\n",
      "   accuracy  precision    recall  f1_score  \\\n",
      "0     0.320   0.295038  0.297607  0.273850   \n",
      "1     0.320   0.295002  0.296524  0.274947   \n",
      "2     0.345   0.289295  0.277037  0.269059   \n",
      "3     0.285   0.306417  0.303875  0.261300   \n",
      "4     0.360   0.318717  0.311111  0.295991   \n",
      "\n",
      "                                         folder_name                date  \\\n",
      "0  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:49:04   \n",
      "1  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:43:41   \n",
      "2  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:38:56   \n",
      "3  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:34:19   \n",
      "4  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:28:34   \n",
      "\n",
      "                                        dataset_name  \\\n",
      "0  fake_news_detection_dataset_cross_lingual_form...   \n",
      "1  fake_news_detection_dataset_cross_lingual_form...   \n",
      "2  fake_news_detection_dataset_cross_lingual_form...   \n",
      "3  fake_news_detection_dataset_cross_lingual_form...   \n",
      "4  fake_news_detection_dataset_cross_lingual_form...   \n",
      "\n",
      "                     split_name           model_name  \n",
      "0  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "1  train_claim_arb_evidence_idn  gemma-2-9b-bnb-4bit  \n",
      "2   train_claim_arb_evidence_en  gemma-2-9b-bnb-4bit  \n",
      "3  train_claim_idn_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "4  train_claim_idn_evidence_idn  gemma-2-9b-bnb-4bit  \n",
      "\n",
      "Merged Results DataFrame:\n",
      "                                                text       true_label  \\\n",
      "0  تقرير IPCC ديال 1990 قال] بأن الكتل الجليدية ف...  NOT_ENOUGH_INFO   \n",
      "1  وكالة الطاقة الدولية، منظمة تحليل عالمية، \"ما ...  NOT_ENOUGH_INFO   \n",
      "2  ارتفاع مستوى سطح البحر كيزيد دابا بسرعة أكبر م...         SUPPORTS   \n",
      "3  \"دابا، الرف الجليدي كيخدم بحال سدادة ديال قرعة...  NOT_ENOUGH_INFO   \n",
      "4  التعديل الأول بدل طريقة حساب درجة حرارة سطح ال...  NOT_ENOUGH_INFO   \n",
      "\n",
      "   predicted_label                                        folder_name  \\\n",
      "0         SUPPORTS  fake_news_detection_dataset_cross_lingual_form...   \n",
      "1          REFUTES  fake_news_detection_dataset_cross_lingual_form...   \n",
      "2  NOT_ENOUGH_INFO  fake_news_detection_dataset_cross_lingual_form...   \n",
      "3          REFUTES  fake_news_detection_dataset_cross_lingual_form...   \n",
      "4  NOT_ENOUGH_INFO  fake_news_detection_dataset_cross_lingual_form...   \n",
      "\n",
      "                 date                                       dataset_name  \\\n",
      "0 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "1 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "2 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "3 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "4 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "\n",
      "                     split_name           model_name  \n",
      "0  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "1  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "2  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "3  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "4  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from supertrainer import SUPERTRAINER_PUBLIC_ROOT\n",
    "\n",
    "# Define the directory containing the folders\n",
    "dataset_dir = os.getenv(SUPERTRAINER_PUBLIC_ROOT) # Replace with the path to your directory\n",
    "\n",
    "# Initialize lists to store metrics and results\n",
    "metrics_list = []\n",
    "results_list = []\n",
    "\n",
    "# Define the filter date\n",
    "date_filter = datetime(2024, 10, 28, 10, 14, 31)\n",
    "\n",
    "# Loop through each folder in the directory\n",
    "for folder_name in os.listdir(dataset_dir):\n",
    "    folder_path = os.path.join(dataset_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Extract date, dataset name, split name, and model name from folder name (assuming the format contains these details)\n",
    "        try:\n",
    "            parts = folder_name.split('-')\n",
    "            dataset_name = parts[0]\n",
    "            split_name = parts[1]\n",
    "            date_str = parts[2]\n",
    "            folder_date = datetime.strptime(date_str, \"%Y%m%d_%H%M%S\")\n",
    "            model_name = '-'.join(parts[3:])\n",
    "        except (ValueError, IndexError):\n",
    "            continue  # Skip folders without a valid format\n",
    "\n",
    "        # Apply date filter\n",
    "        if folder_date <= date_filter:\n",
    "            continue\n",
    "\n",
    "        # Load metrics.json if available\n",
    "        metrics_path = os.path.join(folder_path, \"metrics.json\")\n",
    "        if os.path.exists(metrics_path):\n",
    "            with open(metrics_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metrics = json.load(f)\n",
    "                metrics[\"folder_name\"] = folder_name\n",
    "                metrics[\"date\"] = folder_date\n",
    "                metrics[\"dataset_name\"] = dataset_name\n",
    "                metrics[\"split_name\"] = split_name\n",
    "                metrics[\"model_name\"] = model_name\n",
    "                metrics_list.append(metrics)\n",
    "\n",
    "        # Load results.json if available\n",
    "        results_path = os.path.join(folder_path, \"results.json\")\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                results = json.load(f)\n",
    "                for result in results:\n",
    "                    result[\"folder_name\"] = folder_name\n",
    "                    result[\"date\"] = folder_date\n",
    "                    result[\"dataset_name\"] = dataset_name\n",
    "                    result[\"split_name\"] = split_name\n",
    "                    result[\"model_name\"] = model_name\n",
    "                    results_list.append(result)\n",
    "\n",
    "# Sort metrics and results by date (most recent first)\n",
    "metrics_list = sorted(metrics_list, key=lambda x: x[\"date\"], reverse=True)\n",
    "results_list = sorted(results_list, key=lambda x: x[\"date\"], reverse=True)\n",
    "\n",
    "# Create DataFrames for better visualization and manipulation\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Save merged metrics and results to JSON files\n",
    "merged_metrics_path = os.path.join(dataset_dir, \"merged_metrics.json\")\n",
    "merged_results_path = os.path.join(dataset_dir, \"merged_results.json\")\n",
    "\n",
    "with open(merged_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_list, f, default=str, indent=4)\n",
    "\n",
    "with open(merged_results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_list, f, default=str, indent=4)\n",
    "\n",
    "# Optionally, display the merged DataFrames\n",
    "print(\"Merged Metrics DataFrame:\")\n",
    "print(metrics_df.head())\n",
    "print(\"\\nMerged Results DataFrame:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy                                                      0.4\n",
       "precision                                                 0.54373\n",
       "recall                                                   0.588433\n",
       "f1_score                                                 0.425829\n",
       "folder_name     fake_news_detection_dataset_cross_lingual_form...\n",
       "date                                          2024-10-28 10:14:32\n",
       "dataset_name    fake_news_detection_dataset_cross_lingual_form...\n",
       "split_name                             train_claim_en_evidence_en\n",
       "model_name                             claude-3-5-sonnet-20240620\n",
       "Name: 82, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gemma-2-9b-bnb-4bit', 'gpt-4o-mini',\n",
       "       'bert-base-multilingual-uncased', 'indobert-base-uncased',\n",
       "       'xlm-roberta-base', 'bert-base-arabic', 'Llama-3.2-3B-Instruct',\n",
       "       'mistral-7b-instruct-v0.3-bnb-4bit', 'Qwen2.5-7B-bnb-4bit',\n",
       "       'claude-3-5-sonnet-20240620'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df[\"model_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_based_models = {\"xlm-roberta-base\": \"Bilal\", \"bert-base-arabic\": \"Bilal\", \"bert-base-multilingual-uncased\": \"Erland\", \"indobert-base-uncased\": \"Erland\"}\n",
    "open_llms = {\"Llama-3.2-3B-Instruct\": \"Bilal\", \"mistral-7b-instruct-v0.3-bnb-4bit\": \"Bilal\", \"gemma-2-9b-bnb-4bit\": \"Erland\", \"Qwen2.5-7B-bnb-4bit\": \"Erland\"}\n",
    "closed_llms = {\"gpt-4o-mini\": \"Bilal\", \"claude-3-5-sonnet-20240620\": \"Erland\"}\n",
    "\n",
    "metrics_df[\"person\"] = metrics_df[\"model_name\"].apply(lambda x: bert_based_models.get(x) or open_llms.get(x) or closed_llms.get(x))\n",
    "\n",
    "bilal_df = metrics_df[metrics_df[\"person\"] == \"Bilal\"]\n",
    "erland_df = metrics_df[metrics_df[\"person\"] == \"Erland\"]\n",
    "\n",
    "model_map_to_short = {\n",
    "    \"xlm-roberta-base\": \"XLM-R\",\n",
    "    \"bert-base-arabic\": \"ArabicBERT\",\n",
    "    \"bert-base-multilingual-uncased\": \"mBERT\",\n",
    "    \"indobert-base-uncased\": \"IndoBERT\",\n",
    "    \"Llama-3.2-3B-Instruct\": \"Llama\",\n",
    "    \"mistral-7b-instruct-v0.3-bnb-4bit\": \"Mistral\",\n",
    "    \"gemma-2-9b-bnb-4bit\": \"Gemma\",\n",
    "    \"Qwen2.5-7B-bnb-4bit\": \"Qwen\",\n",
    "    \"gpt-4o-mini\": \"GPT-4o\",\n",
    "    \"claude-3-5-sonnet-20240620\": \"Claude\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bert-base-multilingual-uncased', 'indobert-base-uncased',\n",
       "       'Qwen2.5-7B-bnb-4bit', 'claude-3-5-sonnet-20240620'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erland_df[\"model_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Tables for Bilal's Evaluation:\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Bilal's Evaluation (BERT-Based Models)}\n",
      "    \\label{tab:bert-bilal}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    XLM-R & ar & ar & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & ar & id & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & ar & en & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & id & ar & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & id & id & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & id & en & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & en & ar & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & en & id & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & en & en & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    ArabicBERT & ar & ar & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & ar & id & 0.23 & 0.30 & 0.36 & 0.17 \\\\ \n",
      "    ArabicBERT & ar & en & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & id & ar & 0.23 & 0.08 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & id & id & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & id & en & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & en & ar & 0.23 & 0.24 & 0.35 & 0.15 \\\\ \n",
      "    ArabicBERT & en & id & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & en & en & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Bilal's Evaluation (Open LLMs)}\n",
      "    \\label{tab:open-llms-bilal}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    Llama & ar & ar & 0.30 & 0.33 & 0.34 & 0.28 \\\\ \n",
      "    Llama & ar & id & 0.27 & 0.33 & 0.34 & 0.25 \\\\ \n",
      "    Llama & ar & en & 0.32 & 0.34 & 0.36 & 0.30 \\\\ \n",
      "    Llama & id & ar & 0.31 & 0.36 & 0.38 & 0.29 \\\\ \n",
      "    Llama & id & id & 0.28 & 0.34 & 0.34 & 0.26 \\\\ \n",
      "    Llama & id & en & 0.27 & 0.29 & 0.31 & 0.25 \\\\ \n",
      "    Llama & en & ar & 0.39 & 0.37 & 0.39 & 0.35 \\\\ \n",
      "    Llama & en & id & 0.28 & 0.29 & 0.32 & 0.24 \\\\ \n",
      "    Llama & en & en & 0.29 & 0.31 & 0.37 & 0.29 \\\\ \n",
      "    Mistral & ar & ar & 0.41 & 0.33 & 0.34 & 0.32 \\\\ \n",
      "    Mistral & ar & id & 0.36 & 0.36 & 0.38 & 0.33 \\\\ \n",
      "    Mistral & ar & en & 0.31 & 0.32 & 0.33 & 0.28 \\\\ \n",
      "    Mistral & id & ar & 0.33 & 0.30 & 0.29 & 0.28 \\\\ \n",
      "    Mistral & id & id & 0.29 & 0.34 & 0.34 & 0.27 \\\\ \n",
      "    Mistral & id & en & 0.33 & 0.35 & 0.31 & 0.28 \\\\ \n",
      "    Mistral & en & ar & 0.39 & 0.33 & 0.33 & 0.31 \\\\ \n",
      "    Mistral & en & id & 0.32 & 0.36 & 0.38 & 0.30 \\\\ \n",
      "    Mistral & en & en & 0.34 & 0.33 & 0.33 & 0.30 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Bilal's Evaluation (Closed LLMs)}\n",
      "    \\label{tab:closed-llms-bilal}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    GPT-4o & ar & ar & 0.44 & 0.50 & 0.59 & 0.44 \\\\ \n",
      "    GPT-4o & ar & id & 0.46 & 0.53 & 0.61 & 0.46 \\\\ \n",
      "    GPT-4o & ar & en & 0.38 & 0.49 & 0.59 & 0.39 \\\\ \n",
      "    GPT-4o & id & ar & 0.45 & 0.50 & 0.58 & 0.45 \\\\ \n",
      "    GPT-4o & id & id & 0.46 & 0.53 & 0.62 & 0.46 \\\\ \n",
      "    GPT-4o & id & en & 0.39 & 0.52 & 0.58 & 0.40 \\\\ \n",
      "    GPT-4o & en & ar & 0.49 & 0.52 & 0.62 & 0.49 \\\\ \n",
      "    GPT-4o & en & id & 0.43 & 0.51 & 0.59 & 0.44 \\\\ \n",
      "    GPT-4o & en & en & 0.41 & 0.51 & 0.58 & 0.42 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\n",
      "LaTeX Tables for Erland's Evaluation:\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Erland's Evaluation (BERT-Based Models)}\n",
      "    \\label{tab:bert-erland}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    mBERT & ar & ar & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & ar & id & 0.23 & 0.08 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & ar & en & 0.22 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & id & ar & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & id & id & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & id & en & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & en & ar & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & en & id & 0.22 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & en & en & 0.20 & 0.07 & 0.30 & 0.12 \\\\ \n",
      "    IndoBERT & ar & id & 0.20 & 0.14 & 0.39 & 0.20 \\\\ \n",
      "    IndoBERT & ar & en & 0.21 & 0.13 & 0.37 & 0.19 \\\\ \n",
      "    IndoBERT & id & id & 0.23 & 0.13 & 0.37 & 0.19 \\\\ \n",
      "    IndoBERT & id & en & 0.21 & 0.12 & 0.35 & 0.18 \\\\ \n",
      "    IndoBERT & en & id & 0.21 & 0.13 & 0.38 & 0.20 \\\\ \n",
      "    IndoBERT & en & en & 0.22 & 0.12 & 0.35 & 0.17 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Erland's Evaluation (Open LLMs)}\n",
      "    \\label{tab:open-llms-erland}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    Gemma & ar & ar & 0.32 & 0.30 & 0.30 & 0.27 \\\\ \n",
      "    Gemma & ar & id & 0.32 & 0.30 & 0.30 & 0.27 \\\\ \n",
      "    Gemma & ar & en & 0.34 & 0.29 & 0.28 & 0.27 \\\\ \n",
      "    Gemma & id & ar & 0.28 & 0.31 & 0.30 & 0.26 \\\\ \n",
      "    Gemma & id & id & 0.36 & 0.32 & 0.31 & 0.30 \\\\ \n",
      "    Gemma & id & en & 0.37 & 0.33 & 0.32 & 0.31 \\\\ \n",
      "    Gemma & en & ar & 0.27 & 0.26 & 0.25 & 0.23 \\\\ \n",
      "    Gemma & en & id & 0.31 & 0.30 & 0.30 & 0.27 \\\\ \n",
      "    Gemma & en & en & 0.46 & 0.38 & 0.39 & 0.37 \\\\ \n",
      "    Qwen & ar & ar & 0.34 & 0.31 & 0.30 & 0.30 \\\\ \n",
      "    Qwen & ar & id & 0.36 & 0.30 & 0.29 & 0.28 \\\\ \n",
      "    Qwen & ar & en & 0.41 & 0.33 & 0.34 & 0.32 \\\\ \n",
      "    Qwen & id & ar & 0.43 & 0.34 & 0.34 & 0.32 \\\\ \n",
      "    Qwen & id & id & 0.41 & 0.35 & 0.37 & 0.34 \\\\ \n",
      "    Qwen & id & en & 0.41 & 0.35 & 0.36 & 0.35 \\\\ \n",
      "    Qwen & en & ar & 0.44 & 0.33 & 0.33 & 0.32 \\\\ \n",
      "    Qwen & en & id & 0.37 & 0.31 & 0.31 & 0.30 \\\\ \n",
      "    Qwen & en & en & 0.47 & 0.35 & 0.36 & 0.35 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Erland's Evaluation (Closed LLMs)}\n",
      "    \\label{tab:closed-llms-erland}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    Claude & id & id & 0.38 & 0.53 & 0.59 & 0.40 \\\\ \n",
      "    Claude & id & en & 0.37 & 0.57 & 0.60 & 0.40 \\\\ \n",
      "    Claude & en & ar & 0.48 & 0.55 & 0.62 & 0.49 \\\\ \n",
      "    Claude & en & id & 0.43 & 0.57 & 0.59 & 0.46 \\\\ \n",
      "    Claude & en & en & 0.40 & 0.54 & 0.59 & 0.43 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_individual_latex_tables(df, person_name):\n",
    "    categories = [\n",
    "        (\"BERT-Based Models\", bert_based_models, \"bert\"),\n",
    "        (\"Open LLMs\", open_llms, \"open-llms\"),\n",
    "        (\"Closed LLMs\", closed_llms, \"closed-llms\")\n",
    "    ]\n",
    "    \n",
    "    latex_str = \"\"\n",
    "    \n",
    "    for category_name, models_dict, label_suffix in categories:\n",
    "        latex_str += \"\\\\begin{table}[h]\\n\"\n",
    "        latex_str += f\"    \\\\caption{{Cross-Lingual Evaluation Results of {person_name}'s Evaluation ({category_name})}}\\n\"\n",
    "        latex_str += f\"    \\\\label{{tab:{label_suffix}-{person_name.lower()}}}\\n\"\n",
    "        latex_str += \"    \\\\small\\n\"\n",
    "        latex_str += \"    \\\\begin{tabularx}{\\\\columnwidth}{l l l X X X X}\\n\"\n",
    "        latex_str += \"    \\\\toprule\\n\"\n",
    "        latex_str += \"    \\\\textbf{Model} & \\\\textbf{Claim} & \\\\textbf{Evidence} & \\\\textbf{Acc} & \\\\textbf{Prec} & \\\\textbf{Rec} & \\\\textbf{F1} \\\\\\\\ \\n\"\n",
    "        latex_str += \"    \\\\midrule\\n\"\n",
    "        \n",
    "        # Filter dataframe for models in the current category\n",
    "        category_models = df[df[\"model_name\"].isin(models_dict.keys())]\n",
    "        for _, row in category_models.iterrows():\n",
    "            claim_lang = row[\"split_name\"].split('_')[2]\n",
    "            evidence_lang = row[\"split_name\"].split('_')[4]\n",
    "            claim_lang = \"en\" if claim_lang == \"en\" else \"id\" if claim_lang == \"idn\" else \"ar\"\n",
    "            evidence_lang = \"en\" if evidence_lang == \"en\" else \"id\" if evidence_lang == \"idn\" else \"ar\"\n",
    "            latex_str += f\"    {model_map_to_short[row['model_name']]} & {claim_lang} & {evidence_lang} & {row['accuracy']:.2f} & {row['precision']:.2f} & {row['recall']:.2f} & {row['f1_score']:.2f} \\\\\\\\ \\n\"\n",
    "        \n",
    "        latex_str += \"    \\\\bottomrule\\n\"\n",
    "        latex_str += \"    \\\\end{tabularx}\\n\"\n",
    "        latex_str += \"\\\\end{table}\\n\\n\"\n",
    "    \n",
    "    return latex_str\n",
    "\n",
    "# Generate LaTeX tables for Bilal and Erland\n",
    "latex_bilal_tables = generate_individual_latex_tables(bilal_df, \"Bilal\")\n",
    "latex_erland_tables = generate_individual_latex_tables(erland_df, \"Erland\")\n",
    "\n",
    "# Print the LaTeX code for each person's tables\n",
    "print(\"\\nLaTeX Tables for Bilal's Evaluation:\\n\")\n",
    "print(latex_bilal_tables)\n",
    "print(\"\\nLaTeX Tables for Erland's Evaluation:\\n\")\n",
    "print(latex_erland_tables)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
