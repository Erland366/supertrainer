{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Metrics DataFrame:\n",
      "   accuracy  precision    recall  f1_score  \\\n",
      "0     0.320   0.295038  0.297607  0.273850   \n",
      "1     0.320   0.295002  0.296524  0.274947   \n",
      "2     0.345   0.289295  0.277037  0.269059   \n",
      "3     0.285   0.306417  0.303875  0.261300   \n",
      "4     0.360   0.318717  0.311111  0.295991   \n",
      "\n",
      "                                         folder_name                date  \\\n",
      "0  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:49:04   \n",
      "1  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:43:41   \n",
      "2  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:38:56   \n",
      "3  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:34:19   \n",
      "4  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:28:34   \n",
      "\n",
      "                                        dataset_name  \\\n",
      "0  fake_news_detection_dataset_cross_lingual_form...   \n",
      "1  fake_news_detection_dataset_cross_lingual_form...   \n",
      "2  fake_news_detection_dataset_cross_lingual_form...   \n",
      "3  fake_news_detection_dataset_cross_lingual_form...   \n",
      "4  fake_news_detection_dataset_cross_lingual_form...   \n",
      "\n",
      "                     split_name           model_name  \n",
      "0  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "1  train_claim_arb_evidence_idn  gemma-2-9b-bnb-4bit  \n",
      "2   train_claim_arb_evidence_en  gemma-2-9b-bnb-4bit  \n",
      "3  train_claim_idn_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "4  train_claim_idn_evidence_idn  gemma-2-9b-bnb-4bit  \n",
      "\n",
      "Merged Results DataFrame:\n",
      "                                                text       true_label  \\\n",
      "0  تقرير IPCC ديال 1990 قال] بأن الكتل الجليدية ف...  NOT_ENOUGH_INFO   \n",
      "1  وكالة الطاقة الدولية، منظمة تحليل عالمية، \"ما ...  NOT_ENOUGH_INFO   \n",
      "2  ارتفاع مستوى سطح البحر كيزيد دابا بسرعة أكبر م...         SUPPORTS   \n",
      "3  \"دابا، الرف الجليدي كيخدم بحال سدادة ديال قرعة...  NOT_ENOUGH_INFO   \n",
      "4  التعديل الأول بدل طريقة حساب درجة حرارة سطح ال...  NOT_ENOUGH_INFO   \n",
      "\n",
      "   predicted_label                                        folder_name  \\\n",
      "0         SUPPORTS  fake_news_detection_dataset_cross_lingual_form...   \n",
      "1          REFUTES  fake_news_detection_dataset_cross_lingual_form...   \n",
      "2  NOT_ENOUGH_INFO  fake_news_detection_dataset_cross_lingual_form...   \n",
      "3          REFUTES  fake_news_detection_dataset_cross_lingual_form...   \n",
      "4  NOT_ENOUGH_INFO  fake_news_detection_dataset_cross_lingual_form...   \n",
      "\n",
      "                 date                                       dataset_name  \\\n",
      "0 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "1 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "2 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "3 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "4 2024-10-28 21:49:04  fake_news_detection_dataset_cross_lingual_form...   \n",
      "\n",
      "                     split_name           model_name  \n",
      "0  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "1  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "2  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "3  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n",
      "4  train_claim_arb_evidence_arb  gemma-2-9b-bnb-4bit  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from supertrainer import SUPERTRAINER_PUBLIC_ROOT\n",
    "\n",
    "# Define the directory containing the folders\n",
    "dataset_dir = os.getenv(SUPERTRAINER_PUBLIC_ROOT) # Replace with the path to your directory\n",
    "\n",
    "# Initialize lists to store metrics and results\n",
    "metrics_list = []\n",
    "results_list = []\n",
    "\n",
    "# Define the filter date\n",
    "date_filter = datetime(2024, 10, 28, 10, 14, 31)\n",
    "\n",
    "# Loop through each folder in the directory\n",
    "for folder_name in os.listdir(dataset_dir):\n",
    "    folder_path = os.path.join(dataset_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Extract date, dataset name, split name, and model name from folder name (assuming the format contains these details)\n",
    "        try:\n",
    "            parts = folder_name.split('-')\n",
    "            dataset_name = parts[0]\n",
    "            split_name = parts[1]\n",
    "            date_str = parts[2]\n",
    "            folder_date = datetime.strptime(date_str, \"%Y%m%d_%H%M%S\")\n",
    "            model_name = '-'.join(parts[3:])\n",
    "        except (ValueError, IndexError):\n",
    "            continue  # Skip folders without a valid format\n",
    "\n",
    "        # Apply date filter\n",
    "        if folder_date <= date_filter:\n",
    "            continue\n",
    "\n",
    "        # Load metrics.json if available\n",
    "        metrics_path = os.path.join(folder_path, \"metrics.json\")\n",
    "        if os.path.exists(metrics_path):\n",
    "            with open(metrics_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metrics = json.load(f)\n",
    "                metrics[\"folder_name\"] = folder_name\n",
    "                metrics[\"date\"] = folder_date\n",
    "                metrics[\"dataset_name\"] = dataset_name\n",
    "                metrics[\"split_name\"] = split_name\n",
    "                metrics[\"model_name\"] = model_name\n",
    "                metrics_list.append(metrics)\n",
    "\n",
    "        # Load results.json if available\n",
    "        results_path = os.path.join(folder_path, \"results.json\")\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                results = json.load(f)\n",
    "                for result in results:\n",
    "                    result[\"folder_name\"] = folder_name\n",
    "                    result[\"date\"] = folder_date\n",
    "                    result[\"dataset_name\"] = dataset_name\n",
    "                    result[\"split_name\"] = split_name\n",
    "                    result[\"model_name\"] = model_name\n",
    "                    results_list.append(result)\n",
    "\n",
    "# Sort metrics and results by date (most recent first)\n",
    "metrics_list = sorted(metrics_list, key=lambda x: x[\"date\"], reverse=True)\n",
    "results_list = sorted(results_list, key=lambda x: x[\"date\"], reverse=True)\n",
    "\n",
    "# Create DataFrames for better visualization and manipulation\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Save merged metrics and results to JSON files\n",
    "merged_metrics_path = os.path.join(dataset_dir, \"merged_metrics.json\")\n",
    "merged_results_path = os.path.join(dataset_dir, \"merged_results.json\")\n",
    "\n",
    "with open(merged_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_list, f, default=str, indent=4)\n",
    "\n",
    "with open(merged_results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_list, f, default=str, indent=4)\n",
    "\n",
    "# Optionally, display the merged DataFrames\n",
    "print(\"Merged Metrics DataFrame:\")\n",
    "print(metrics_df.head())\n",
    "print(\"\\nMerged Results DataFrame:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy                                                      0.4\n",
       "precision                                                 0.54373\n",
       "recall                                                   0.588433\n",
       "f1_score                                                 0.425829\n",
       "folder_name     fake_news_detection_dataset_cross_lingual_form...\n",
       "date                                          2024-10-28 10:14:32\n",
       "dataset_name    fake_news_detection_dataset_cross_lingual_form...\n",
       "split_name                             train_claim_en_evidence_en\n",
       "model_name                             claude-3-5-sonnet-20240620\n",
       "Name: 82, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gemma-2-9b-bnb-4bit', 'gpt-4o-mini',\n",
       "       'bert-base-multilingual-uncased', 'indobert-base-uncased',\n",
       "       'xlm-roberta-base', 'bert-base-arabic', 'Llama-3.2-3B-Instruct',\n",
       "       'mistral-7b-instruct-v0.3-bnb-4bit', 'Qwen2.5-7B-bnb-4bit',\n",
       "       'claude-3-5-sonnet-20240620'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df[\"model_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_based_models = {\"xlm-roberta-base\": \"Bilal\", \"bert-base-arabic\": \"Bilal\", \"bert-base-multilingual-uncased\": \"Erland\", \"indobert-base-uncased\": \"Erland\"}\n",
    "open_llms = {\"Llama-3.2-3B-Instruct\": \"Bilal\", \"mistral-7b-instruct-v0.3-bnb-4bit\": \"Bilal\", \"gemma-2-9b-bnb-4bit\": \"Erland\", \"Qwen2.5-7B-bnb-4bit\": \"Erland\"}\n",
    "closed_llms = {\"gpt-4o-mini\": \"Bilal\", \"claude-3-5-sonnet-20240620\": \"Erland\"}\n",
    "\n",
    "metrics_df[\"person\"] = metrics_df[\"model_name\"].apply(lambda x: bert_based_models.get(x) or open_llms.get(x) or closed_llms.get(x))\n",
    "\n",
    "bilal_df = metrics_df[metrics_df[\"person\"] == \"Bilal\"]\n",
    "erland_df = metrics_df[metrics_df[\"person\"] == \"Erland\"]\n",
    "\n",
    "model_map_to_short = {\n",
    "    \"xlm-roberta-base\": \"XLM-R\",\n",
    "    \"bert-base-arabic\": \"ArabicBERT\",\n",
    "    \"bert-base-multilingual-uncased\": \"mBERT\",\n",
    "    \"indobert-base-uncased\": \"IndoBERT\",\n",
    "    \"Llama-3.2-3B-Instruct\": \"Llama\",\n",
    "    \"mistral-7b-instruct-v0.3-bnb-4bit\": \"Mistral\",\n",
    "    \"gemma-2-9b-bnb-4bit\": \"Gemma\",\n",
    "    \"Qwen2.5-7B-bnb-4bit\": \"Qwen\",\n",
    "    \"gpt-4o-mini\": \"GPT-4o\",\n",
    "    \"claude-3-5-sonnet-20240620\": \"Claude\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Model categorization\n",
    "bert_based_models = {\n",
    "    \"xlm-roberta-base\": \"BERT-based\",\n",
    "    \"bert-base-arabic\": \"BERT-based\", \n",
    "    \"bert-base-multilingual-uncased\": \"BERT-based\",\n",
    "    \"indobert-base-uncased\": \"BERT-based\"\n",
    "}\n",
    "\n",
    "open_llms = {\n",
    "    \"Llama-3.2-3B-Instruct\": \"Open LLM\",\n",
    "    \"mistral-7b-instruct-v0.3-bnb-4bit\": \"Open LLM\",\n",
    "    \"gemma-2-9b-bnb-4bit\": \"Open LLM\",\n",
    "    \"Qwen2.5-7B-bnb-4bit\": \"Open LLM\"\n",
    "}\n",
    "\n",
    "closed_llms = {\n",
    "    \"gpt-4o-mini\": \"Closed LLM\",\n",
    "    \"claude-3-5-sonnet-20240620\": \"Closed LLM\"\n",
    "}\n",
    "\n",
    "# Model sizes (in billions of parameters) - approximate values\n",
    "model_sizes = {\n",
    "    \"xlm-roberta-base\": 0.125,\n",
    "    \"bert-base-arabic\": 0.11,\n",
    "    \"bert-base-multilingual-uncased\": 0.11,\n",
    "    \"indobert-base-uncased\": 0.11,\n",
    "    \"Llama-3.2-3B-Instruct\": 3,\n",
    "    \"mistral-7b-instruct-v0.3-bnb-4bit\": 7,\n",
    "    \"gemma-2-9b-bnb-4bit\": 2.9,\n",
    "    \"Qwen2.5-7B-bnb-4bit\": 7,\n",
    "    \"gpt-4o-mini\": 20,  \n",
    "    \"claude-3-5-sonnet-20240620\": 20  \n",
    "}\n",
    "\n",
    "# Combine all models into a dictionary with their categories\n",
    "all_models = {**bert_based_models, **open_llms, **closed_llms}\n",
    "\n",
    "def process_data(df):\n",
    "    # Filter for English evidence only\n",
    "    df_en = df[df['folder_name'].str.contains('evidence_en')]\n",
    "    \n",
    "    # Add model category and size\n",
    "    df_en['model_category'] = df_en['model_name'].map(lambda x: [v for k,v in all_models.items() if k in x][0])\n",
    "    df_en['model_size'] = df_en['model_name'].map(lambda x: model_sizes[[k for k in model_sizes.keys() if k in x][0]])\n",
    "    \n",
    "    # Extract claim language\n",
    "    df_en['claim_language'] = df_en['folder_name'].str.extract('claim_(\\w+)_evidence')\n",
    "    \n",
    "    return df_en\n",
    "\n",
    "def plot_performance_by_category(df_en):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    x = np.arange(len(df_en['model_category'].unique()))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        means = df_en.groupby('model_category')[metric].mean()\n",
    "        plt.bar(x + i*width, means, width, label=metric.replace('_', ' ').title())\n",
    "    \n",
    "    plt.xlabel('Model Category')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance by Category (English Evidence)')\n",
    "    plt.xticks(x + width*1.5, df_en['model_category'].unique(), rotation=0)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('category_performance.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_language_comparison(df_en):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    sns.boxplot(data=df_en, x='claim_language', y='f1_score', hue='model_category')\n",
    "    plt.title('F1 Score by Claim Language and Model Category')\n",
    "    plt.xlabel('Claim Language')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('language_comparison.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_size_impact(df_en):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    sns.scatterplot(data=df_en, x='model_size', y='f1_score', \n",
    "                    hue='model_category', style='claim_language',\n",
    "                    s=100, alpha=0.7)\n",
    "    \n",
    "    plt.xscale('log')  # Log scale for model size\n",
    "    plt.title('Impact of Model Size on Performance')\n",
    "    plt.xlabel('Model Size (Billion Parameters)')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('size_impact.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Assuming df is your original DataFrame\n",
    "# df_en = process_data(metrics_df)\n",
    "# plot_performance_by_category(df_en)\n",
    "# plot_language_comparison(df_en)\n",
    "# plot_size_impact(df_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>folder_name</th>\n",
       "      <th>date</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>split_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.320</td>\n",
       "      <td>0.295038</td>\n",
       "      <td>0.297607</td>\n",
       "      <td>0.273850</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 21:49:04</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_arb</td>\n",
       "      <td>gemma-2-9b-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.320</td>\n",
       "      <td>0.295002</td>\n",
       "      <td>0.296524</td>\n",
       "      <td>0.274947</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 21:43:41</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_idn</td>\n",
       "      <td>gemma-2-9b-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.345</td>\n",
       "      <td>0.289295</td>\n",
       "      <td>0.277037</td>\n",
       "      <td>0.269059</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 21:38:56</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>gemma-2-9b-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.285</td>\n",
       "      <td>0.306417</td>\n",
       "      <td>0.303875</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 21:34:19</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_arb</td>\n",
       "      <td>gemma-2-9b-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.360</td>\n",
       "      <td>0.318717</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.295991</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 21:28:34</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_idn</td>\n",
       "      <td>gemma-2-9b-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.380</td>\n",
       "      <td>0.530741</td>\n",
       "      <td>0.587863</td>\n",
       "      <td>0.404891</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 11:36:01</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_idn</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.370</td>\n",
       "      <td>0.565988</td>\n",
       "      <td>0.599430</td>\n",
       "      <td>0.395954</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 11:10:11</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.485</td>\n",
       "      <td>0.552472</td>\n",
       "      <td>0.620171</td>\n",
       "      <td>0.492959</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 10:51:42</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_arb</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.430</td>\n",
       "      <td>0.568176</td>\n",
       "      <td>0.588205</td>\n",
       "      <td>0.461038</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 10:33:23</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_idn</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.543730</td>\n",
       "      <td>0.588433</td>\n",
       "      <td>0.425829</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 10:14:32</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Erland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  precision    recall  f1_score  \\\n",
       "0      0.320   0.295038  0.297607  0.273850   \n",
       "1      0.320   0.295002  0.296524  0.274947   \n",
       "2      0.345   0.289295  0.277037  0.269059   \n",
       "3      0.285   0.306417  0.303875  0.261300   \n",
       "4      0.360   0.318717  0.311111  0.295991   \n",
       "..       ...        ...       ...       ...   \n",
       "78     0.380   0.530741  0.587863  0.404891   \n",
       "79     0.370   0.565988  0.599430  0.395954   \n",
       "80     0.485   0.552472  0.620171  0.492959   \n",
       "81     0.430   0.568176  0.588205  0.461038   \n",
       "82     0.400   0.543730  0.588433  0.425829   \n",
       "\n",
       "                                          folder_name                date  \\\n",
       "0   fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:49:04   \n",
       "1   fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:43:41   \n",
       "2   fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:38:56   \n",
       "3   fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:34:19   \n",
       "4   fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:28:34   \n",
       "..                                                ...                 ...   \n",
       "78  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 11:36:01   \n",
       "79  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 11:10:11   \n",
       "80  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 10:51:42   \n",
       "81  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 10:33:23   \n",
       "82  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 10:14:32   \n",
       "\n",
       "                                         dataset_name  \\\n",
       "0   fake_news_detection_dataset_cross_lingual_form...   \n",
       "1   fake_news_detection_dataset_cross_lingual_form...   \n",
       "2   fake_news_detection_dataset_cross_lingual_form...   \n",
       "3   fake_news_detection_dataset_cross_lingual_form...   \n",
       "4   fake_news_detection_dataset_cross_lingual_form...   \n",
       "..                                                ...   \n",
       "78  fake_news_detection_dataset_cross_lingual_form...   \n",
       "79  fake_news_detection_dataset_cross_lingual_form...   \n",
       "80  fake_news_detection_dataset_cross_lingual_form...   \n",
       "81  fake_news_detection_dataset_cross_lingual_form...   \n",
       "82  fake_news_detection_dataset_cross_lingual_form...   \n",
       "\n",
       "                      split_name                  model_name  person  \n",
       "0   train_claim_arb_evidence_arb         gemma-2-9b-bnb-4bit  Erland  \n",
       "1   train_claim_arb_evidence_idn         gemma-2-9b-bnb-4bit  Erland  \n",
       "2    train_claim_arb_evidence_en         gemma-2-9b-bnb-4bit  Erland  \n",
       "3   train_claim_idn_evidence_arb         gemma-2-9b-bnb-4bit  Erland  \n",
       "4   train_claim_idn_evidence_idn         gemma-2-9b-bnb-4bit  Erland  \n",
       "..                           ...                         ...     ...  \n",
       "78  train_claim_idn_evidence_idn  claude-3-5-sonnet-20240620  Erland  \n",
       "79   train_claim_idn_evidence_en  claude-3-5-sonnet-20240620  Erland  \n",
       "80   train_claim_en_evidence_arb  claude-3-5-sonnet-20240620  Erland  \n",
       "81   train_claim_en_evidence_idn  claude-3-5-sonnet-20240620  Erland  \n",
       "82    train_claim_en_evidence_en  claude-3-5-sonnet-20240620  Erland  \n",
       "\n",
       "[83 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_processed_dataframes(df):\n",
    "    \"\"\"\n",
    "    Process the original dataframe to create filtered and summary dataframes\n",
    "    \"\"\"\n",
    "    # Step 1: Create a model category mapping\n",
    "    model_category_map = {**bert_based_models, **open_llms, **closed_llms}\n",
    "    \n",
    "    # Step 2: Filter for only English evidence\n",
    "    mask_en_evidence = df['folder_name'].str.contains('evidence_en')\n",
    "    df_filtered = df[mask_en_evidence].copy()\n",
    "    \n",
    "    # Step 3: Add model category and size\n",
    "    df_filtered['model_category'] = df_filtered['model_name'].map(\n",
    "        lambda x: next((v for k, v in model_category_map.items() if k in x), \"Unknown\")\n",
    "    )\n",
    "    df_filtered['model_size'] = df_filtered['model_name'].map(\n",
    "        lambda x: next((v for k, v in model_sizes.items() if k in x), 0)\n",
    "    )\n",
    "    \n",
    "    # Step 4: Extract claim language\n",
    "    df_filtered['claim_language'] = df_filtered['folder_name'].str.extract('claim_(\\w+)_evidence')\n",
    "    \n",
    "    # Step 5: Create summary dataframe\n",
    "    df_summary = pd.DataFrame()\n",
    "    \n",
    "    # Calculate average performance for English and non-English claims\n",
    "    for model in df_filtered['model_name'].unique():\n",
    "        model_data = df_filtered[df_filtered['model_name'] == model]\n",
    "        \n",
    "        en_perf = model_data[model_data['claim_language'] == 'en']['f1_score'].iloc[0]\n",
    "        non_en_perf = model_data[model_data['claim_language'] != 'en']['f1_score'].mean()\n",
    "        \n",
    "        model_category = model_data['model_category'].iloc[0]\n",
    "        model_size = model_data['model_size'].iloc[0]\n",
    "        \n",
    "        # Get metrics from the English claim data\n",
    "        precision = model_data[model_data['claim_language'] == 'en']['precision'].iloc[0]\n",
    "        recall = model_data[model_data['claim_language'] == 'en']['recall'].iloc[0]\n",
    "        \n",
    "        df_summary = pd.concat([df_summary, pd.DataFrame({\n",
    "            'model_name': [model],\n",
    "            'model_category': [model_category],\n",
    "            'model_size': [model_size],\n",
    "            'en_performance': [en_perf],\n",
    "            'avg_non_en_performance': [non_en_perf],\n",
    "            'precision': [precision],\n",
    "            'recall': [recall],\n",
    "            'avg_f1': [model_data['f1_score'].mean()]\n",
    "        })])\n",
    "    \n",
    "    return df_filtered, df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_category</th>\n",
       "      <th>model_size</th>\n",
       "      <th>en_performance</th>\n",
       "      <th>non_en_performance</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>avg_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma-2-9b-bnb-4bit</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>2.900</td>\n",
       "      <td>0.372865</td>\n",
       "      <td>0.288436</td>\n",
       "      <td>0.375214</td>\n",
       "      <td>0.389972</td>\n",
       "      <td>0.316579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>Closed LLM</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.419517</td>\n",
       "      <td>0.396073</td>\n",
       "      <td>0.506056</td>\n",
       "      <td>0.577265</td>\n",
       "      <td>0.403888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-multilingual-uncased</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.115331</td>\n",
       "      <td>0.121334</td>\n",
       "      <td>0.071181</td>\n",
       "      <td>0.303704</td>\n",
       "      <td>0.119333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>indobert-base-uncased</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.185277</td>\n",
       "      <td>0.123950</td>\n",
       "      <td>0.349630</td>\n",
       "      <td>0.181131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-arabic</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.122449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.285641</td>\n",
       "      <td>0.273346</td>\n",
       "      <td>0.310890</td>\n",
       "      <td>0.365128</td>\n",
       "      <td>0.277444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-7b-instruct-v0.3-bnb-4bit</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>7.000</td>\n",
       "      <td>0.296002</td>\n",
       "      <td>0.282162</td>\n",
       "      <td>0.331160</td>\n",
       "      <td>0.326154</td>\n",
       "      <td>0.286776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2.5-7B-bnb-4bit</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>7.000</td>\n",
       "      <td>0.347996</td>\n",
       "      <td>0.332806</td>\n",
       "      <td>0.354342</td>\n",
       "      <td>0.359430</td>\n",
       "      <td>0.337869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Closed LLM</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.425829</td>\n",
       "      <td>0.395954</td>\n",
       "      <td>0.543730</td>\n",
       "      <td>0.588433</td>\n",
       "      <td>0.410892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model_name model_category  model_size  \\\n",
       "0                gemma-2-9b-bnb-4bit       Open LLM       2.900   \n",
       "0                        gpt-4o-mini     Closed LLM      20.000   \n",
       "0     bert-base-multilingual-uncased     BERT-based       0.110   \n",
       "0              indobert-base-uncased     BERT-based       0.110   \n",
       "0                   xlm-roberta-base     BERT-based       0.125   \n",
       "0                   bert-base-arabic     BERT-based       0.110   \n",
       "0              Llama-3.2-3B-Instruct       Open LLM       3.000   \n",
       "0  mistral-7b-instruct-v0.3-bnb-4bit       Open LLM       7.000   \n",
       "0                Qwen2.5-7B-bnb-4bit       Open LLM       7.000   \n",
       "0         claude-3-5-sonnet-20240620     Closed LLM      20.000   \n",
       "\n",
       "   en_performance  non_en_performance  precision    recall    avg_f1  \n",
       "0        0.372865            0.288436   0.375214  0.389972  0.316579  \n",
       "0        0.419517            0.396073   0.506056  0.577265  0.403888  \n",
       "0        0.115331            0.121334   0.071181  0.303704  0.119333  \n",
       "0        0.172840            0.185277   0.123950  0.349630  0.181131  \n",
       "0        0.074074            0.074074   0.041667  0.333333  0.074074  \n",
       "0        0.122449            0.122449   0.075000  0.333333  0.122449  \n",
       "0        0.285641            0.273346   0.310890  0.365128  0.277444  \n",
       "0        0.296002            0.282162   0.331160  0.326154  0.286776  \n",
       "0        0.347996            0.332806   0.354342  0.359430  0.337869  \n",
       "0        0.425829            0.395954   0.543730  0.588433  0.410892  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>folder_name</th>\n",
       "      <th>date</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>split_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>person</th>\n",
       "      <th>model_category</th>\n",
       "      <th>model_size</th>\n",
       "      <th>claim_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.345</td>\n",
       "      <td>0.289295</td>\n",
       "      <td>0.277037</td>\n",
       "      <td>0.269059</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 21:38:56</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>gemma-2-9b-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>2.900</td>\n",
       "      <td>arb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.370</td>\n",
       "      <td>0.330929</td>\n",
       "      <td>0.321083</td>\n",
       "      <td>0.307813</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 21:23:46</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>gemma-2-9b-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>2.900</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.375214</td>\n",
       "      <td>0.389972</td>\n",
       "      <td>0.372865</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 21:09:02</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>gemma-2-9b-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>2.900</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.380</td>\n",
       "      <td>0.486184</td>\n",
       "      <td>0.590541</td>\n",
       "      <td>0.389355</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 19:41:04</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>Closed LLM</td>\n",
       "      <td>20.000</td>\n",
       "      <td>arb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.390</td>\n",
       "      <td>0.521879</td>\n",
       "      <td>0.582222</td>\n",
       "      <td>0.402792</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 19:24:07</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>Closed LLM</td>\n",
       "      <td>20.000</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.415</td>\n",
       "      <td>0.506056</td>\n",
       "      <td>0.577265</td>\n",
       "      <td>0.419517</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 19:06:40</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>Closed LLM</td>\n",
       "      <td>20.000</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.220</td>\n",
       "      <td>0.073702</td>\n",
       "      <td>0.325926</td>\n",
       "      <td>0.120219</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:54:28</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>bert-base-multilingual-uncased</td>\n",
       "      <td>Erland</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>arb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.225</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:53:47</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>bert-base-multilingual-uncased</td>\n",
       "      <td>Erland</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.205</td>\n",
       "      <td>0.071181</td>\n",
       "      <td>0.303704</td>\n",
       "      <td>0.115331</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:53:06</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>bert-base-multilingual-uncased</td>\n",
       "      <td>Erland</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.215</td>\n",
       "      <td>0.131639</td>\n",
       "      <td>0.371852</td>\n",
       "      <td>0.194238</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:51:41</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>indobert-base-uncased</td>\n",
       "      <td>Erland</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>arb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.210</td>\n",
       "      <td>0.119122</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.176316</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:50:54</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>indobert-base-uncased</td>\n",
       "      <td>Erland</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.220</td>\n",
       "      <td>0.123950</td>\n",
       "      <td>0.349630</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:49:48</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>indobert-base-uncased</td>\n",
       "      <td>Erland</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:48:51</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.125</td>\n",
       "      <td>arb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:48:09</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.125</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:47:23</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.125</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.225</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:46:39</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>bert-base-arabic</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>arb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.225</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:45:57</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>bert-base-arabic</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.225</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:45:17</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>bert-base-arabic</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>0.110</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.320</td>\n",
       "      <td>0.340392</td>\n",
       "      <td>0.355726</td>\n",
       "      <td>0.295283</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:42:00</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>3.000</td>\n",
       "      <td>arb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.265</td>\n",
       "      <td>0.294582</td>\n",
       "      <td>0.307066</td>\n",
       "      <td>0.251409</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:37:44</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>3.000</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.290</td>\n",
       "      <td>0.310890</td>\n",
       "      <td>0.365128</td>\n",
       "      <td>0.285641</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:33:18</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>3.000</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.310</td>\n",
       "      <td>0.318836</td>\n",
       "      <td>0.330142</td>\n",
       "      <td>0.281706</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:24:47</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>mistral-7b-instruct-v0.3-bnb-4bit</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>7.000</td>\n",
       "      <td>arb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.345260</td>\n",
       "      <td>0.312536</td>\n",
       "      <td>0.282619</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:16:24</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>mistral-7b-instruct-v0.3-bnb-4bit</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>7.000</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.340</td>\n",
       "      <td>0.331160</td>\n",
       "      <td>0.326154</td>\n",
       "      <td>0.296002</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 12:07:53</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>mistral-7b-instruct-v0.3-bnb-4bit</td>\n",
       "      <td>Bilal</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>7.000</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.405</td>\n",
       "      <td>0.332820</td>\n",
       "      <td>0.336866</td>\n",
       "      <td>0.318941</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 11:59:39</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_arb_evidence_en</td>\n",
       "      <td>Qwen2.5-7B-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>7.000</td>\n",
       "      <td>arb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.415</td>\n",
       "      <td>0.354187</td>\n",
       "      <td>0.360855</td>\n",
       "      <td>0.346671</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 11:53:30</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>Qwen2.5-7B-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>7.000</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.470</td>\n",
       "      <td>0.354342</td>\n",
       "      <td>0.359430</td>\n",
       "      <td>0.347996</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 11:47:22</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>Qwen2.5-7B-bnb-4bit</td>\n",
       "      <td>Erland</td>\n",
       "      <td>Open LLM</td>\n",
       "      <td>7.000</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.370</td>\n",
       "      <td>0.565988</td>\n",
       "      <td>0.599430</td>\n",
       "      <td>0.395954</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 11:10:11</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_idn_evidence_en</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Erland</td>\n",
       "      <td>Closed LLM</td>\n",
       "      <td>20.000</td>\n",
       "      <td>idn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.543730</td>\n",
       "      <td>0.588433</td>\n",
       "      <td>0.425829</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>2024-10-28 10:14:32</td>\n",
       "      <td>fake_news_detection_dataset_cross_lingual_form...</td>\n",
       "      <td>train_claim_en_evidence_en</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Erland</td>\n",
       "      <td>Closed LLM</td>\n",
       "      <td>20.000</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  precision    recall  f1_score  \\\n",
       "2      0.345   0.289295  0.277037  0.269059   \n",
       "5      0.370   0.330929  0.321083  0.307813   \n",
       "8      0.455   0.375214  0.389972  0.372865   \n",
       "11     0.380   0.486184  0.590541  0.389355   \n",
       "14     0.390   0.521879  0.582222  0.402792   \n",
       "17     0.415   0.506056  0.577265  0.419517   \n",
       "20     0.220   0.073702  0.325926  0.120219   \n",
       "23     0.225   0.075000  0.333333  0.122449   \n",
       "26     0.205   0.071181  0.303704  0.115331   \n",
       "28     0.215   0.131639  0.371852  0.194238   \n",
       "30     0.210   0.119122  0.346667  0.176316   \n",
       "32     0.220   0.123950  0.349630  0.172840   \n",
       "35     0.125   0.041667  0.333333  0.074074   \n",
       "38     0.125   0.041667  0.333333  0.074074   \n",
       "41     0.125   0.041667  0.333333  0.074074   \n",
       "44     0.225   0.075000  0.333333  0.122449   \n",
       "47     0.225   0.075000  0.333333  0.122449   \n",
       "50     0.225   0.075000  0.333333  0.122449   \n",
       "53     0.320   0.340392  0.355726  0.295283   \n",
       "56     0.265   0.294582  0.307066  0.251409   \n",
       "59     0.290   0.310890  0.365128  0.285641   \n",
       "62     0.310   0.318836  0.330142  0.281706   \n",
       "65     0.325   0.345260  0.312536  0.282619   \n",
       "68     0.340   0.331160  0.326154  0.296002   \n",
       "71     0.405   0.332820  0.336866  0.318941   \n",
       "74     0.415   0.354187  0.360855  0.346671   \n",
       "77     0.470   0.354342  0.359430  0.347996   \n",
       "79     0.370   0.565988  0.599430  0.395954   \n",
       "82     0.400   0.543730  0.588433  0.425829   \n",
       "\n",
       "                                          folder_name                date  \\\n",
       "2   fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:38:56   \n",
       "5   fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:23:46   \n",
       "8   fake_news_detection_dataset_cross_lingual_form... 2024-10-28 21:09:02   \n",
       "11  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 19:41:04   \n",
       "14  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 19:24:07   \n",
       "17  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 19:06:40   \n",
       "20  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:54:28   \n",
       "23  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:53:47   \n",
       "26  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:53:06   \n",
       "28  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:51:41   \n",
       "30  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:50:54   \n",
       "32  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:49:48   \n",
       "35  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:48:51   \n",
       "38  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:48:09   \n",
       "41  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:47:23   \n",
       "44  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:46:39   \n",
       "47  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:45:57   \n",
       "50  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:45:17   \n",
       "53  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:42:00   \n",
       "56  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:37:44   \n",
       "59  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:33:18   \n",
       "62  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:24:47   \n",
       "65  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:16:24   \n",
       "68  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 12:07:53   \n",
       "71  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 11:59:39   \n",
       "74  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 11:53:30   \n",
       "77  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 11:47:22   \n",
       "79  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 11:10:11   \n",
       "82  fake_news_detection_dataset_cross_lingual_form... 2024-10-28 10:14:32   \n",
       "\n",
       "                                         dataset_name  \\\n",
       "2   fake_news_detection_dataset_cross_lingual_form...   \n",
       "5   fake_news_detection_dataset_cross_lingual_form...   \n",
       "8   fake_news_detection_dataset_cross_lingual_form...   \n",
       "11  fake_news_detection_dataset_cross_lingual_form...   \n",
       "14  fake_news_detection_dataset_cross_lingual_form...   \n",
       "17  fake_news_detection_dataset_cross_lingual_form...   \n",
       "20  fake_news_detection_dataset_cross_lingual_form...   \n",
       "23  fake_news_detection_dataset_cross_lingual_form...   \n",
       "26  fake_news_detection_dataset_cross_lingual_form...   \n",
       "28  fake_news_detection_dataset_cross_lingual_form...   \n",
       "30  fake_news_detection_dataset_cross_lingual_form...   \n",
       "32  fake_news_detection_dataset_cross_lingual_form...   \n",
       "35  fake_news_detection_dataset_cross_lingual_form...   \n",
       "38  fake_news_detection_dataset_cross_lingual_form...   \n",
       "41  fake_news_detection_dataset_cross_lingual_form...   \n",
       "44  fake_news_detection_dataset_cross_lingual_form...   \n",
       "47  fake_news_detection_dataset_cross_lingual_form...   \n",
       "50  fake_news_detection_dataset_cross_lingual_form...   \n",
       "53  fake_news_detection_dataset_cross_lingual_form...   \n",
       "56  fake_news_detection_dataset_cross_lingual_form...   \n",
       "59  fake_news_detection_dataset_cross_lingual_form...   \n",
       "62  fake_news_detection_dataset_cross_lingual_form...   \n",
       "65  fake_news_detection_dataset_cross_lingual_form...   \n",
       "68  fake_news_detection_dataset_cross_lingual_form...   \n",
       "71  fake_news_detection_dataset_cross_lingual_form...   \n",
       "74  fake_news_detection_dataset_cross_lingual_form...   \n",
       "77  fake_news_detection_dataset_cross_lingual_form...   \n",
       "79  fake_news_detection_dataset_cross_lingual_form...   \n",
       "82  fake_news_detection_dataset_cross_lingual_form...   \n",
       "\n",
       "                     split_name                         model_name  person  \\\n",
       "2   train_claim_arb_evidence_en                gemma-2-9b-bnb-4bit  Erland   \n",
       "5   train_claim_idn_evidence_en                gemma-2-9b-bnb-4bit  Erland   \n",
       "8    train_claim_en_evidence_en                gemma-2-9b-bnb-4bit  Erland   \n",
       "11  train_claim_arb_evidence_en                        gpt-4o-mini   Bilal   \n",
       "14  train_claim_idn_evidence_en                        gpt-4o-mini   Bilal   \n",
       "17   train_claim_en_evidence_en                        gpt-4o-mini   Bilal   \n",
       "20  train_claim_arb_evidence_en     bert-base-multilingual-uncased  Erland   \n",
       "23  train_claim_idn_evidence_en     bert-base-multilingual-uncased  Erland   \n",
       "26   train_claim_en_evidence_en     bert-base-multilingual-uncased  Erland   \n",
       "28  train_claim_arb_evidence_en              indobert-base-uncased  Erland   \n",
       "30  train_claim_idn_evidence_en              indobert-base-uncased  Erland   \n",
       "32   train_claim_en_evidence_en              indobert-base-uncased  Erland   \n",
       "35  train_claim_arb_evidence_en                   xlm-roberta-base   Bilal   \n",
       "38  train_claim_idn_evidence_en                   xlm-roberta-base   Bilal   \n",
       "41   train_claim_en_evidence_en                   xlm-roberta-base   Bilal   \n",
       "44  train_claim_arb_evidence_en                   bert-base-arabic   Bilal   \n",
       "47  train_claim_idn_evidence_en                   bert-base-arabic   Bilal   \n",
       "50   train_claim_en_evidence_en                   bert-base-arabic   Bilal   \n",
       "53  train_claim_arb_evidence_en              Llama-3.2-3B-Instruct   Bilal   \n",
       "56  train_claim_idn_evidence_en              Llama-3.2-3B-Instruct   Bilal   \n",
       "59   train_claim_en_evidence_en              Llama-3.2-3B-Instruct   Bilal   \n",
       "62  train_claim_arb_evidence_en  mistral-7b-instruct-v0.3-bnb-4bit   Bilal   \n",
       "65  train_claim_idn_evidence_en  mistral-7b-instruct-v0.3-bnb-4bit   Bilal   \n",
       "68   train_claim_en_evidence_en  mistral-7b-instruct-v0.3-bnb-4bit   Bilal   \n",
       "71  train_claim_arb_evidence_en                Qwen2.5-7B-bnb-4bit  Erland   \n",
       "74  train_claim_idn_evidence_en                Qwen2.5-7B-bnb-4bit  Erland   \n",
       "77   train_claim_en_evidence_en                Qwen2.5-7B-bnb-4bit  Erland   \n",
       "79  train_claim_idn_evidence_en         claude-3-5-sonnet-20240620  Erland   \n",
       "82   train_claim_en_evidence_en         claude-3-5-sonnet-20240620  Erland   \n",
       "\n",
       "   model_category  model_size claim_language  \n",
       "2        Open LLM       2.900            arb  \n",
       "5        Open LLM       2.900            idn  \n",
       "8        Open LLM       2.900             en  \n",
       "11     Closed LLM      20.000            arb  \n",
       "14     Closed LLM      20.000            idn  \n",
       "17     Closed LLM      20.000             en  \n",
       "20     BERT-based       0.110            arb  \n",
       "23     BERT-based       0.110            idn  \n",
       "26     BERT-based       0.110             en  \n",
       "28     BERT-based       0.110            arb  \n",
       "30     BERT-based       0.110            idn  \n",
       "32     BERT-based       0.110             en  \n",
       "35     BERT-based       0.125            arb  \n",
       "38     BERT-based       0.125            idn  \n",
       "41     BERT-based       0.125             en  \n",
       "44     BERT-based       0.110            arb  \n",
       "47     BERT-based       0.110            idn  \n",
       "50     BERT-based       0.110             en  \n",
       "53       Open LLM       3.000            arb  \n",
       "56       Open LLM       3.000            idn  \n",
       "59       Open LLM       3.000             en  \n",
       "62       Open LLM       7.000            arb  \n",
       "65       Open LLM       7.000            idn  \n",
       "68       Open LLM       7.000             en  \n",
       "71       Open LLM       7.000            arb  \n",
       "74       Open LLM       7.000            idn  \n",
       "77       Open LLM       7.000             en  \n",
       "79     Closed LLM      20.000            idn  \n",
       "82     Closed LLM      20.000             en  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "\n",
    "def create_bubble_plot(df, x_metric, y_metric, title, filename, label_offsets=None):\n",
    "    \"\"\"\n",
    "    Create bubble plot with manual control over label positions\n",
    "    \n",
    "    Parameters:\n",
    "    - label_offsets: dict with model names as keys and (x_offset, y_offset) as values\n",
    "                    e.g., {'Claude': (0.02, 0.01), 'GPT-4': (-0.02, 0.01)}\n",
    "    \"\"\"\n",
    "    if label_offsets is None:\n",
    "        label_offsets = {}\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 12,          # Base font size\n",
    "        'axes.titlesize': 16,     # Title font size\n",
    "        'axes.labelsize': 14,     # Axis label font size\n",
    "        'xtick.labelsize': 12,    # X-axis tick labels\n",
    "        'ytick.labelsize': 12,    # Y-axis tick labels\n",
    "        'legend.title_fontsize': 8,  # Legend title font size\n",
    "        'legend.fontsize': 8     # Legend text font size\n",
    "    })\n",
    "    \n",
    "    # Create color mapping for categories\n",
    "    categories = ['BERT-based', 'Open LLM', 'Closed LLM']\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "    color_map = dict(zip(categories, colors))\n",
    "    \n",
    "    # Create scatter plot\n",
    "    dummy_scatter_plot = []\n",
    "    for category in categories:\n",
    "        mask = df['model_category'] == category\n",
    "        \n",
    "        # Calculate bubble sizes\n",
    "        sizes = df[mask]['model_size'] * 100\n",
    "        \n",
    "        plt.scatter(df[mask][x_metric], \n",
    "                   df[mask][y_metric],\n",
    "                   s=sizes,\n",
    "                   c=[color_map[category]],\n",
    "                   alpha=0.6,\n",
    "                   label=category)\n",
    "\n",
    "        # Create a dummy scatter plot with uniform size for legend\n",
    "        dummy_scatter = plt.scatter([], [], \n",
    "                                  s=200,  # Fixed size for legend\n",
    "                                  c=[color_map[category]],\n",
    "                                  alpha=0.6,\n",
    "                                  label=category)\n",
    "        dummy_scatter_plot.append(dummy_scatter)\n",
    "        \n",
    "        # Add model name labels\n",
    "        for idx, row in df[mask].iterrows():\n",
    "            model_name = model_map_to_short[row['model_name']]\n",
    "            \n",
    "            x_offset, y_offset = label_offsets.get(model_name, (5, 5))\n",
    "        \n",
    "            plt.annotate(model_name,\n",
    "                        (row[x_metric], row[y_metric]),\n",
    "                        xytext=(x_offset, y_offset),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=12)\n",
    "    \n",
    "    plt.xlabel(x_metric.replace('_', ' ').title())\n",
    "    plt.ylabel(y_metric.replace('_', ' ').title())\n",
    "    plt.title(title)\n",
    "\n",
    "    # Set axes limits from 0 to 1\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Create legend with uniform-sized markers\n",
    "    plt.legend(dummy_scatter_plot,\n",
    "              categories,\n",
    "              title=\"Model Category\",\n",
    "              bbox_to_anchor=(1.05, 1.05),\n",
    "              loc='lower right',\n",
    "              borderaxespad=0.5,\n",
    "              framealpha=0.9,\n",
    "              labelspacing=1.5)\n",
    "    \n",
    "    \n",
    "    # Add size legend with increased spacing\n",
    "    size_legend_elements = []\n",
    "    sizes = [0.1, 3, 7, 20]\n",
    "    labels = ['0.1B params', '3B params', '7B params', 'Proprietary (>7B params)']\n",
    "    \n",
    "    for size, label in zip(sizes, labels):\n",
    "        size_legend_elements.append(\n",
    "            plt.scatter([], [], s=size*100, c='gray', alpha=0.3, label=label)\n",
    "        )\n",
    "    \n",
    "    # legend2 = plt.legend(handles=size_legend_elements,\n",
    "    #                     title=\"Model Size\",\n",
    "    #                     bbox_to_anchor=(1.05, 0.5),\n",
    "    #                     loc='center left',\n",
    "    #                     borderaxespad=0,\n",
    "    #                     labelspacing=5,\n",
    "    #                     handletextpad=5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, format='pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Example usage:\n",
    "# Define custom offsets for specific models\n",
    "\n",
    "\n",
    "\n",
    "# Create processed dataframes\n",
    "df_filtered, df_summary = create_processed_dataframes(metrics_df)\n",
    "\n",
    "# Now you can use these with the bubble plot function:\n",
    "label_offsets = {\n",
    "    # 'Claude': (10, 10),      # Move Claude label 10 points right, 10 points up\n",
    "    # 'GPT-4': (-10, 5),       # Move GPT-4 label 10 points left, 5 points up\n",
    "    # 'Llama': (5, -5),        # Move Llama label 5 points right, 5 points down\n",
    "    # 'Mistral': (-5, -10),    # Move Mistral label 5 points left, 10 points down\n",
    "    # 'Qwen': (0, 5),          # Move Qwen label 15 points up\n",
    "    \"XLM-R\" : (-5, -10),\n",
    "    \"IndoBERT\" : (0, 10),\n",
    "    # \"ArabicBERT\": (0, 10),\n",
    "    \"mBERT\" : (0, -10),\n",
    "    \"ArabicBERT\" : (0, 10)\n",
    "}\n",
    "create_bubble_plot(df_summary, \n",
    "                  'precision', \n",
    "                  'recall',\n",
    "                  'Precision-Recall Trade-off by Model Type and Size',\n",
    "                  'precision_recall_bubble_adjusted.pdf',\n",
    "                  label_offsets=label_offsets)\n",
    "\n",
    "label_offsets = {\n",
    "    'Claude': (10, 10),      # Move Claude label 10 points right, 10 points up\n",
    "    'GPT-4': (-10, 5),       # Move GPT-4 label 10 points left, 5 points up\n",
    "    # 'Llama': (5, -5),        # Move Llama label 5 points right, 5 points down\n",
    "    'Mistral': (-5, -10),    # Move Mistral label 5 points left, 10 points down\n",
    "    'Qwen': (0, 5),          # Move Qwen label 15 points up\n",
    "    \"XLM-R\" : (-5, 10),\n",
    "    \"ArabicBERT\": (0, 10),\n",
    "}\n",
    "\n",
    "create_bubble_plot(df_summary, \n",
    "                  'en_performance', \n",
    "                  'avg_non_en_performance',\n",
    "                  'Cross-lingual Performance by Model Type and Size',\n",
    "                  'crosslingual_bubble.pdf',\n",
    "                  label_offsets=label_offsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bert-base-multilingual-uncased', 'indobert-base-uncased',\n",
       "       'Qwen2.5-7B-bnb-4bit', 'claude-3-5-sonnet-20240620'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erland_df[\"model_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_context(\"paper\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Performance Comparison across Metrics\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "x = np.arange(len(metrics_df['model_name']))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.bar(x + i*width, metrics_df[metric], width, label=metric.capitalize())\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison Across All Metrics')\n",
    "plt.xticks(x + width*1.5, metrics_df['model_name'], rotation=45, ha='right')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 2. Language Pair Performance Heatmap\n",
    "# Assuming you have language pair data\n",
    "# Create a pivot table for language pairs\n",
    "lang_pairs = ['en-en', 'en-arb', 'en-idn', 'arb-arb', 'arb-en', 'arb-idn', 'idn-idn', 'idn-en', 'idn-arb']\n",
    "languages = ['English', 'Arabic', 'Indonesian']\n",
    "performance_matrix = np.random.rand(3, 3)  # Replace with actual data\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(performance_matrix, annot=True, fmt='.3f', \n",
    "            xticklabels=languages, yticklabels=languages,\n",
    "            cmap='YlOrRd', vmin=0, vmax=1)\n",
    "plt.title('Cross-lingual Performance Matrix (F1-Score)')\n",
    "plt.xlabel('Evidence Language')\n",
    "plt.ylabel('Claim Language')\n",
    "plt.tight_layout()\n",
    "plt.savefig('language_heatmap.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 3. Performance Distribution Box Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_melted = metrics_df.melt(id_vars=['model_name'], value_vars=metrics, \n",
    "                    var_name='Metric', value_name='Score')\n",
    "\n",
    "sns.boxplot(data=df_melted, x='Metric', y='Score', width=0.5)\n",
    "plt.title('Distribution of Model Performance by Metric')\n",
    "plt.xlabel('Evaluation Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig('metric_distribution.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Tables for Bilal's Evaluation:\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Bilal's Evaluation (BERT-Based Models)}\n",
      "    \\label{tab:bert-bilal}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    XLM-R & ar & ar & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & ar & id & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & ar & en & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & id & ar & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & id & id & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & id & en & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & en & ar & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & en & id & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    XLM-R & en & en & 0.12 & 0.04 & 0.33 & 0.07 \\\\ \n",
      "    ArabicBERT & ar & ar & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & ar & id & 0.23 & 0.30 & 0.36 & 0.17 \\\\ \n",
      "    ArabicBERT & ar & en & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & id & ar & 0.23 & 0.08 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & id & id & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & id & en & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & en & ar & 0.23 & 0.24 & 0.35 & 0.15 \\\\ \n",
      "    ArabicBERT & en & id & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    ArabicBERT & en & en & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Bilal's Evaluation (Open LLMs)}\n",
      "    \\label{tab:open-llms-bilal}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    Llama & ar & ar & 0.30 & 0.33 & 0.34 & 0.28 \\\\ \n",
      "    Llama & ar & id & 0.27 & 0.33 & 0.34 & 0.25 \\\\ \n",
      "    Llama & ar & en & 0.32 & 0.34 & 0.36 & 0.30 \\\\ \n",
      "    Llama & id & ar & 0.31 & 0.36 & 0.38 & 0.29 \\\\ \n",
      "    Llama & id & id & 0.28 & 0.34 & 0.34 & 0.26 \\\\ \n",
      "    Llama & id & en & 0.27 & 0.29 & 0.31 & 0.25 \\\\ \n",
      "    Llama & en & ar & 0.39 & 0.37 & 0.39 & 0.35 \\\\ \n",
      "    Llama & en & id & 0.28 & 0.29 & 0.32 & 0.24 \\\\ \n",
      "    Llama & en & en & 0.29 & 0.31 & 0.37 & 0.29 \\\\ \n",
      "    Mistral & ar & ar & 0.41 & 0.33 & 0.34 & 0.32 \\\\ \n",
      "    Mistral & ar & id & 0.36 & 0.36 & 0.38 & 0.33 \\\\ \n",
      "    Mistral & ar & en & 0.31 & 0.32 & 0.33 & 0.28 \\\\ \n",
      "    Mistral & id & ar & 0.33 & 0.30 & 0.29 & 0.28 \\\\ \n",
      "    Mistral & id & id & 0.29 & 0.34 & 0.34 & 0.27 \\\\ \n",
      "    Mistral & id & en & 0.33 & 0.35 & 0.31 & 0.28 \\\\ \n",
      "    Mistral & en & ar & 0.39 & 0.33 & 0.33 & 0.31 \\\\ \n",
      "    Mistral & en & id & 0.32 & 0.36 & 0.38 & 0.30 \\\\ \n",
      "    Mistral & en & en & 0.34 & 0.33 & 0.33 & 0.30 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Bilal's Evaluation (Closed LLMs)}\n",
      "    \\label{tab:closed-llms-bilal}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    GPT-4o & ar & ar & 0.44 & 0.50 & 0.59 & 0.44 \\\\ \n",
      "    GPT-4o & ar & id & 0.46 & 0.53 & 0.61 & 0.46 \\\\ \n",
      "    GPT-4o & ar & en & 0.38 & 0.49 & 0.59 & 0.39 \\\\ \n",
      "    GPT-4o & id & ar & 0.45 & 0.50 & 0.58 & 0.45 \\\\ \n",
      "    GPT-4o & id & id & 0.46 & 0.53 & 0.62 & 0.46 \\\\ \n",
      "    GPT-4o & id & en & 0.39 & 0.52 & 0.58 & 0.40 \\\\ \n",
      "    GPT-4o & en & ar & 0.49 & 0.52 & 0.62 & 0.49 \\\\ \n",
      "    GPT-4o & en & id & 0.43 & 0.51 & 0.59 & 0.44 \\\\ \n",
      "    GPT-4o & en & en & 0.41 & 0.51 & 0.58 & 0.42 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "\n",
      "LaTeX Tables for Erland's Evaluation:\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Erland's Evaluation (BERT-Based Models)}\n",
      "    \\label{tab:bert-erland}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    mBERT & ar & ar & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & ar & id & 0.23 & 0.08 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & ar & en & 0.22 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & id & ar & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & id & id & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & id & en & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & en & ar & 0.23 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & en & id & 0.22 & 0.07 & 0.33 & 0.12 \\\\ \n",
      "    mBERT & en & en & 0.20 & 0.07 & 0.30 & 0.12 \\\\ \n",
      "    IndoBERT & ar & id & 0.20 & 0.14 & 0.39 & 0.20 \\\\ \n",
      "    IndoBERT & ar & en & 0.21 & 0.13 & 0.37 & 0.19 \\\\ \n",
      "    IndoBERT & id & id & 0.23 & 0.13 & 0.37 & 0.19 \\\\ \n",
      "    IndoBERT & id & en & 0.21 & 0.12 & 0.35 & 0.18 \\\\ \n",
      "    IndoBERT & en & id & 0.21 & 0.13 & 0.38 & 0.20 \\\\ \n",
      "    IndoBERT & en & en & 0.22 & 0.12 & 0.35 & 0.17 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Erland's Evaluation (Open LLMs)}\n",
      "    \\label{tab:open-llms-erland}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    Gemma & ar & ar & 0.32 & 0.30 & 0.30 & 0.27 \\\\ \n",
      "    Gemma & ar & id & 0.32 & 0.30 & 0.30 & 0.27 \\\\ \n",
      "    Gemma & ar & en & 0.34 & 0.29 & 0.28 & 0.27 \\\\ \n",
      "    Gemma & id & ar & 0.28 & 0.31 & 0.30 & 0.26 \\\\ \n",
      "    Gemma & id & id & 0.36 & 0.32 & 0.31 & 0.30 \\\\ \n",
      "    Gemma & id & en & 0.37 & 0.33 & 0.32 & 0.31 \\\\ \n",
      "    Gemma & en & ar & 0.27 & 0.26 & 0.25 & 0.23 \\\\ \n",
      "    Gemma & en & id & 0.31 & 0.30 & 0.30 & 0.27 \\\\ \n",
      "    Gemma & en & en & 0.46 & 0.38 & 0.39 & 0.37 \\\\ \n",
      "    Qwen & ar & ar & 0.34 & 0.31 & 0.30 & 0.30 \\\\ \n",
      "    Qwen & ar & id & 0.36 & 0.30 & 0.29 & 0.28 \\\\ \n",
      "    Qwen & ar & en & 0.41 & 0.33 & 0.34 & 0.32 \\\\ \n",
      "    Qwen & id & ar & 0.43 & 0.34 & 0.34 & 0.32 \\\\ \n",
      "    Qwen & id & id & 0.41 & 0.35 & 0.37 & 0.34 \\\\ \n",
      "    Qwen & id & en & 0.41 & 0.35 & 0.36 & 0.35 \\\\ \n",
      "    Qwen & en & ar & 0.44 & 0.33 & 0.33 & 0.32 \\\\ \n",
      "    Qwen & en & id & 0.37 & 0.31 & 0.31 & 0.30 \\\\ \n",
      "    Qwen & en & en & 0.47 & 0.35 & 0.36 & 0.35 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[h]\n",
      "    \\caption{Cross-Lingual Evaluation Results of Erland's Evaluation (Closed LLMs)}\n",
      "    \\label{tab:closed-llms-erland}\n",
      "    \\small\n",
      "    \\begin{tabularx}{\\columnwidth}{l l l X X X X}\n",
      "    \\toprule\n",
      "    \\textbf{Model} & \\textbf{Claim} & \\textbf{Evidence} & \\textbf{Acc} & \\textbf{Prec} & \\textbf{Rec} & \\textbf{F1} \\\\ \n",
      "    \\midrule\n",
      "    Claude & id & id & 0.38 & 0.53 & 0.59 & 0.40 \\\\ \n",
      "    Claude & id & en & 0.37 & 0.57 & 0.60 & 0.40 \\\\ \n",
      "    Claude & en & ar & 0.48 & 0.55 & 0.62 & 0.49 \\\\ \n",
      "    Claude & en & id & 0.43 & 0.57 & 0.59 & 0.46 \\\\ \n",
      "    Claude & en & en & 0.40 & 0.54 & 0.59 & 0.43 \\\\ \n",
      "    \\bottomrule\n",
      "    \\end{tabularx}\n",
      "\\end{table}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_individual_latex_tables(df, person_name):\n",
    "    categories = [\n",
    "        (\"BERT-Based Models\", bert_based_models, \"bert\"),\n",
    "        (\"Open LLMs\", open_llms, \"open-llms\"),\n",
    "        (\"Closed LLMs\", closed_llms, \"closed-llms\")\n",
    "    ]\n",
    "    \n",
    "    latex_str = \"\"\n",
    "    \n",
    "    for category_name, models_dict, label_suffix in categories:\n",
    "        latex_str += \"\\\\begin{table}[h]\\n\"\n",
    "        latex_str += f\"    \\\\caption{{Cross-Lingual Evaluation Results of {person_name}'s Evaluation ({category_name})}}\\n\"\n",
    "        latex_str += f\"    \\\\label{{tab:{label_suffix}-{person_name.lower()}}}\\n\"\n",
    "        latex_str += \"    \\\\small\\n\"\n",
    "        latex_str += \"    \\\\begin{tabularx}{\\\\columnwidth}{l l l X X X X}\\n\"\n",
    "        latex_str += \"    \\\\toprule\\n\"\n",
    "        latex_str += \"    \\\\textbf{Model} & \\\\textbf{Claim} & \\\\textbf{Evidence} & \\\\textbf{Acc} & \\\\textbf{Prec} & \\\\textbf{Rec} & \\\\textbf{F1} \\\\\\\\ \\n\"\n",
    "        latex_str += \"    \\\\midrule\\n\"\n",
    "        \n",
    "        # Filter dataframe for models in the current category\n",
    "        category_models = df[df[\"model_name\"].isin(models_dict.keys())]\n",
    "        for _, row in category_models.iterrows():\n",
    "            claim_lang = row[\"split_name\"].split('_')[2]\n",
    "            evidence_lang = row[\"split_name\"].split('_')[4]\n",
    "            claim_lang = \"en\" if claim_lang == \"en\" else \"id\" if claim_lang == \"idn\" else \"ar\"\n",
    "            evidence_lang = \"en\" if evidence_lang == \"en\" else \"id\" if evidence_lang == \"idn\" else \"ar\"\n",
    "            latex_str += f\"    {model_map_to_short[row['model_name']]} & {claim_lang} & {evidence_lang} & {row['accuracy']:.2f} & {row['precision']:.2f} & {row['recall']:.2f} & {row['f1_score']:.2f} \\\\\\\\ \\n\"\n",
    "        \n",
    "        latex_str += \"    \\\\bottomrule\\n\"\n",
    "        latex_str += \"    \\\\end{tabularx}\\n\"\n",
    "        latex_str += \"\\\\end{table}\\n\\n\"\n",
    "    \n",
    "    return latex_str\n",
    "\n",
    "# Generate LaTeX tables for Bilal and Erland\n",
    "latex_bilal_tables = generate_individual_latex_tables(bilal_df, \"Bilal\")\n",
    "latex_erland_tables = generate_individual_latex_tables(erland_df, \"Erland\")\n",
    "\n",
    "# Print the LaTeX code for each person's tables\n",
    "print(\"\\nLaTeX Tables for Bilal's Evaluation:\\n\")\n",
    "print(latex_bilal_tables)\n",
    "print(\"\\nLaTeX Tables for Erland's Evaluation:\\n\")\n",
    "print(latex_erland_tables)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
