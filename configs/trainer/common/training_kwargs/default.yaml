gradient_accumulation_steps: 64
warmup_steps: 5
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 1
learning_rate: 0.00002
fp16: false
bf16: true
logging_steps: 20
logging_strategy: "steps"
optim: "adamw_8bit"
weight_decay: 0.01
lr_scheduler_type: "linear"
seed: 3407
report_to: "wandb"
output_dir: "./assets_local/run_outputs"
# Experiment on eval params
load_best_model_at_end: 1
metric_for_best_model: "eval_loss"
save_total_limit: 2
eval_strategy: "steps"
eval_steps: 200
save_steps: 400
eval_on_start: True
# hub stuff
hub_model_id: "Erland/training_run"
hub_private_repo: True
hub_strategy: "every_save"
push_to_hub: True
# Experimental
auto_find_batch_size: True
include_tokens_per_second: True
include_num_input_tokens_seen: True
# Different from unsloth here
gradient_checkpointing: True
gradient_checkpointing_kwargs:
  # THIS IS MANDATORY
  use_reentrant: False
