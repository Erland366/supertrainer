# @package _global_

defaults:
- /experiments/fact_checking/default
- _self_

trainer:
  peft_kwargs:
    r: 64
    lora_alpha: 128
    lora_dropout: 0.1
    task_type: SEQ_CLS
    use_dora: False
    init_lora_weights: "gaussian"
  training_kwargs:
    per_device_train_batch_size: 16
    hub_model_id: "Erland/xlmr-fact-checking"

  class_name: supertrainer.trainers.xlmr.XLMRTrainer
  model_name: FacebookAI/xlm-roberta-base
