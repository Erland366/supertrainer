# @package _global_

defaults:
- /experiments/fact_checking/default
- _self_

trainer:
  peft_kwargs:
    r: 64
    lora_alpha: 128
    lora_dropout: 0.1
    task_type: SEQ_CLS
    use_dora: False
    init_lora_weights: "gaussian"
  training_kwargs:
    per_device_train_batch_size: 16
    hub_model_id: "Erland/mbert-fact-checking-combineddata"

  class_name: supertrainer.trainers.mbert.mBERTTrainer
  model_name: google-bert/bert-base-multilingual-uncased
