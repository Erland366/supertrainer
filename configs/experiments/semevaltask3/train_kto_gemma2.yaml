# @package _global_

defaults:
- /experiments/semevaltask3/default
- override /dataset/llm@dataset: semevaltask3_kto
- _self_

trainer:
  peft_kwargs:
    r: 64
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    lora_alpha: 128
    lora_dropout: 0
    bias: "none"
    use_gradient_checkpointing: "unsloth"
    random_state: 3407
    use_rslora: false
    loftq_config: null
  training_kwargs:
    hub_model_id: "Erland/gemma2-semeval-kto-task3"
    per_device_train_batch_size: 2
  processor_kwargs:

  class_name: supertrainer.trainers.gemma2.Gemma2KTOTrainer
  model_name: unsloth/gemma-2-9b-bnb-4bit
  max_seq_length: 4096

dataset:
  chat_template: gemma2_chatml
