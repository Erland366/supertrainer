# @package _global_

defaults:
- /experiments/soft_robotics/default
- _self_

trainer:
  peft_kwargs:
    r: 8
    lora_alpha: 8
    lora_dropout: 0.1
    target_modules:
      - 'down_proj'
      - 'o_proj'
      - 'k_proj'
      - 'q_proj'
      - 'gate_proj'
      - 'up_proj'
      - 'v_proj'
    use_dora: False
    init_lora_weights: "gaussian"
  training_kwargs:
    num_train_epochs: 1
    per_device_train_batch_size: 4
    hub_model_id: "Erland/idefics3-soft-robotics"
    eval_strategy: null # disable first
    load_best_model_at_end: False
    eval_on_start: False
    remove_unused_columns: False
    auto_find_batch_size: False

  processor_kwargs:
  class_name: supertrainer.trainers.idefics3.Idefics3Trainer
  model_name: HuggingFaceM4/Idefics3-8B-Llama3

dataset:
  data_collator_class_name: supertrainer.data.soft_robotics.Idefics3DataCollator
